# ‚ö†Ô∏è DEPRECATED - This Document Has Been Consolidated

> **üö® Important Update**: This document has been consolidated into the new comprehensive reference. Please use the single reference link below instead of this scattered document.
>
> **üìã NEW SINGLE REFERENCE**: [Flink.NET Backpressure: Complete Reference Guide](Backpressure-Complete-Reference.md)
>
> **Why the change?**: Users reported the backpressure wiki was "messy" with too many scattered documents. We've consolidated everything into one comprehensive guide that covers:
> - Performance guidance (when to enable/disable rate limiting)
> - Scalability architecture (multiple consumers, logical queues)
> - Unique identifier strategy and partition relationships
> - Rebalancing integration and best practices
> - Industry patterns with scholarly references
> - Complete implementation guide with examples
>
> **üéØ This gives you everything in one place instead of hunting through multiple documents.**

---

# Backpressure Aspire Container Architecture Wiki

**How Flink.NET Aspire orchestrates distributed backpressure across containers**

> **üö® Flink JobManager Compatibility Note**: This documentation has been updated to use Flink JobManager-compatible patterns. Rate limiting code uses synchronous methods (`rateLimiter.TryAcquire()`) and `ConfigureAwait(false)` for async operations to ensure reliable distributed execution. See [Flink JobManager Async Compatibility Guide](../flink-jobmanager-async-compatibility.md) for details.

This wiki explains how the Flink.NET Aspire project starts, how all containers are configured, and how they work together to implement comprehensive backpressure management.

## Table of Contents

1. [Complete Beginner's Guide](#complete-beginners-guide) üöÄ **START HERE**
2. [Overview](#overview)
3. [How Aspire Project Starts](#how-aspire-project-starts)
4. [Container Architecture](#container-architecture)  
5. [ReqNRoll BDD Testing with Aspire](#reqnroll-bdd-testing-with-aspire) ‚≠ê **NEW**
6. [Backpressure Flow Between Containers](#backpressure-flow-between-containers)
7. [Container Interaction Patterns](#container-interaction-patterns)
8. [Monitoring and Observability](#monitoring-and-observability)
9. [Production Deployment](#production-deployment)
10. [Troubleshooting](#troubleshooting)

## Complete Beginner's Guide

> **üöÄ START HERE if you have never used Kafka or Flink before!**

### What is Backpressure? (In Simple Terms)

Imagine you have a **fast water hose** filling up a **small bucket**. If you don't control the water flow, the bucket will overflow and make a mess. **Backpressure** is like a smart valve that automatically slows down the water when the bucket gets full.

In computer terms:
- **Water hose** = Data coming into your system (like messages, requests, etc.)
- **Bucket** = Your system's ability to process that data
- **Smart valve** = Backpressure system that prevents overload

### What Are These Components? (No Technical Jargon)

Think of this system like a **smart restaurant** that never gets overwhelmed:

**üçΩÔ∏è Kafka** = The **Order Queue**
- Takes customer orders and keeps them organized
- Makes sure no orders get lost
- Can handle thousands of orders at once

**üë®‚Äçüíº Flink** = The **Kitchen Manager**  
- Manages multiple cooks (workers) 
- Decides which cook handles which order
- Watches how busy everyone is

**üö™ Job Gateway** = The **Host/Hostess**
- Greets customers and controls how many enter
- Slows down seating when kitchen is busy
- Keeps track of restaurant health

**üìù Redis** = The **Restaurant's Memory**
- Remembers customer preferences
- Keeps track of how busy things are
- Stores important information quickly

### How to Set Up Backpressure (Complete Beginner Steps)

**Step 1: Make Sure You Have the Right Tools**
```bash
# You need these installed on your computer:
# - Docker Desktop (like a container for all the restaurant parts)
# - .NET 8 (the language everything is written in)
```

**Step 2: Get the Code**
```bash
# Download the project to your computer
git clone https://github.com/devstress/FLINK.NET.git
cd FLINK.NET
```

**Step 3: Start Everything With One Command**
```bash
# Go to the main folder
cd Sample/FlinkDotNet.Aspire.AppHost

# Start the entire restaurant (all containers)
dotnet run
```

**Step 4: Wait for Everything to Start**
You'll see messages like:
```
‚úÖ Kafka is ready (Order Queue is working)
‚úÖ Flink is ready (Kitchen Manager is working)  
‚úÖ Gateway is ready (Host is working)
‚úÖ Redis is ready (Memory is working)
```

**Step 5: Open the Dashboard**
```bash
# The system will tell you a web address like:
# "Dashboard: http://localhost:15000"
# Open this in your web browser
```

### What You'll See When It's Working

**‚úÖ Green Lights Everywhere**
- All services show "Healthy" status
- No red error messages

**‚úÖ Data Flowing Smoothly**  
- Messages being processed (like orders being completed)
- No warnings about overload

**‚úÖ Automatic Slowdown When Busy**
- System automatically reduces incoming data when busy
- No crashes or errors

### Simple Test: See Backpressure in Action

**Step 1: Send Some Test Data**
```bash
# Use the built-in test (like sending fake orders to the restaurant)
cd Sample/FlinkDotNet.Aspire.IntegrationTests
dotnet test --filter "Category=backpressure_test"
```

**Step 2: Watch What Happens**
- **Low Load**: Everything processes quickly
- **High Load**: System automatically slows down input
- **Overload**: System politely says "please wait" instead of crashing

**What the test does:**
- Sends 1 million test messages through the system
- Watches how the system handles the load
- Shows you backpressure working in real-time

### When Something Goes Wrong (Simple Troubleshooting)

**‚ùå If containers won't start:**
```bash
# Make sure Docker is running
# Check if you have enough memory (8GB+ recommended)
# Restart Docker Desktop
```

**‚ùå If you see red errors:**
```bash
# Look at the dashboard - it will show which part failed
# Usually just restart that one container
# Check the logs for simple error messages
```

**‚ùå If data stops flowing:**
```bash
# Check if any containers are "Unhealthy"
# Restart the whole system: Stop (Ctrl+C) then "dotnet run" again
```

### What Makes This System Special?

**üéØ It Never Crashes**
- Traditional systems break when overloaded
- This one slows down instead of breaking

**üîÑ It Self-Heals**
- If one part fails, others keep working
- Failed parts automatically restart

**üìä You Can Watch Everything**
- Real-time dashboard shows what's happening
- No guessing if things are working

**‚ö° It's Fast**
- Can handle 900,000+ messages per second
- Automatically uses all your computer's power

### What's Next?

Once you have this running:

1. **Play with the dashboard** - Click around and explore
2. **Run the tests** - See how backpressure works in action  
3. **Try breaking things** - Stop containers and watch them recover
4. **Read the detailed sections below** - Learn more technical details

**üéâ Congratulations! You now have a professional-grade streaming system with automatic backpressure running on your computer!**

---

## Enhanced Rate Limiting Implementation (.NET Code Examples)

### üöÄ NEW: Advanced Rate Limiting Following Flink 2.0 Patterns

> **üìã Critical Context**: The code examples below are for **normal .NET producer/consumer applications**, NOT for Flink job serialization. This code runs in your application processes and coordinates with the Aspire-managed containers.

Flink.NET now includes comprehensive rate limiting implementation based on Apache Flink 2.0 AsyncSink patterns. This section shows practical .NET code examples for implementing effective backpressure with rate limiting.

#### Reference Implementation

Based on: [Optimising the throughput of async sinks using a custom RateLimitingStrategy](https://flink.apache.org/2022/11/25/optimising-the-throughput-of-async-sinks-using-a-custom-ratelimitingstrategy/)

**üìÅ Core Files:**
- Interface: `FlinkDotNet/Flink.JobBuilder/Backpressure/IRateLimitingStrategy.cs`
- Token Bucket: `FlinkDotNet/Flink.JobBuilder/Backpressure/TokenBucketRateLimiter.cs`
- Buffer Pool: `FlinkDotNet/Flink.JobBuilder/Backpressure/BufferPool.cs`
- Multi-Tier: `FlinkDotNet/Flink.JobBuilder/Backpressure/MultiTierRateLimiter.cs`
- Demo: `FlinkDotNet/Flink.JobBuilder/Demo/RateLimitingDemo.cs`

### üîå Backend Connection Configuration

**Where to define connection strings for Aspire containers:**

```csharp
// In your .NET application (not in Flink jobs)
public class MessageProcessorService
{
    private readonly TokenBucketRateLimiter _rateLimiter;
    
    public MessageProcessorService(IConfiguration configuration)
    {
        // Connect to Aspire-managed Kafka cluster
        var kafkaBootstrapServers = configuration["Kafka:BootstrapServers"] ?? "kafka-kraft-cluster:9092";
        
        var kafkaConfig = new KafkaConfig
        {
            BootstrapServers = kafkaBootstrapServers
        };
        
        // Rate limiter uses same Kafka backend as Aspire containers
        _rateLimiter = RateLimiterFactory.CreateWithKafkaStorage(
            rateLimit: 1000.0,
            burstCapacity: 2000.0,
            kafkaConfig: kafkaConfig
        );
    }
}
```

**Configuration in appsettings.json:**

```json
{
  "Kafka": {
    "BootstrapServers": "kafka-kraft-cluster:9092"
  },
  "Redis": {
    "ConnectionString": "redis:6379"
  },
  "Flink": {
    "JobManagerUrl": "http://flink-jobmanager:8081",
    "JobGatewayUrl": "http://flink-job-gateway:8080"
  }
}
```

### 1. Token Bucket Rate Limiter (Burst Capacity)

```csharp
using Flink.JobBuilder.Backpressure;

// Connect to Aspire-managed Kafka cluster for distributed state
var kafkaConfig = new KafkaConfig
{
    BootstrapServers = "kafka-kraft-cluster:9092" // Aspire container name
};

// Create rate limiter with Kafka backend (same as Aspire uses for messaging)
var rateLimiter = RateLimiterFactory.CreateWithKafkaStorage(
    rateLimit: 1000.0,      // Sustained rate
    burstCapacity: 2000.0,  // Burst tolerance
    kafkaConfig: kafkaConfig
);

// Try to acquire permits (Flink JobManager compatible - non-blocking)
if (rateLimiter.TryAcquire(permits: 5))
{
    // Process 5 messages
    ProcessMessages(messages);
}

// For batch processing, use non-blocking approach in Flink jobs
if (rateLimiter.TryAcquire(permits: 10))
{
    ProcessBatch(batch);
}
else
{
    _logger.LogWarning("Rate limited - batch processing delayed");
}

// Monitor utilization
Console.WriteLine($"Utilization: {rateLimiter.CurrentUtilization:P1}");
Console.WriteLine($"Tokens remaining: {rateLimiter.CurrentTokens:F1}");
```

### 2. Buffer Pool with Size and Time Thresholds

```csharp
// Buffer pool: 1000 items max, 5 second max age
var bufferPool = new BufferPool<Message>(
    maxSize: 1000,
    maxAge: TimeSpan.FromSeconds(5),
    rateLimiter: new TokenBucketRateLimiter(100, 200)
);

// Handle flush events
bufferPool.OnFlush += async items =>
{
    Console.WriteLine($"Flushing {items.Length} items");
    await ProcessBatch(items.Select(i => i.Item));
};

// Handle backpressure events  
bufferPool.OnBackpressure += evt =>
{
    Console.WriteLine($"Backpressure: {evt.Reason} at {evt.Utilization:P1}");
    ApplyUpstreamBackpressure();
};

// Add items (use ConfigureAwait for Flink compatibility)
foreach (var message in messages)
{
    await bufferPool.AddAsync(message).ConfigureAwait(false); // Use ConfigureAwait for distributed execution
}
```

### 3. Multi-Tier Rate Limiting (Hierarchical Enforcement)

```csharp
var multiTierLimiter = new MultiTierRateLimiter();

// Configure hierarchical tiers
var tiers = new[]
{
    new RateLimitingTier 
    { 
        Name = "Global", 
        Scope = "Entire cluster", 
        RateLimit = 10_000_000,    // 10M ops/sec
        BurstCapacity = 15_000_000,
        Enforcement = RateLimitingEnforcement.HardLimit
    },
    new RateLimitingTier 
    { 
        Name = "Topic", 
        Scope = "Per topic", 
        RateLimit = 1_000_000,     // 1M ops/sec per topic
        BurstCapacity = 1_500_000,
        Enforcement = RateLimitingEnforcement.Throttling
    },
    new RateLimitingTier 
    { 
        Name = "Consumer", 
        Scope = "Per instance", 
        RateLimit = 100_000,       // 100K ops/sec per consumer
        BurstCapacity = 150_000,
        Enforcement = RateLimitingEnforcement.Backpressure
    }
};

multiTierLimiter.ConfigureTiers(tiers);

// Request context for rate limiting
var context = new RateLimitingContext
{
    TopicName = "high-volume-topic",
    ConsumerGroup = "processing-group",
    ConsumerId = "consumer-1",
    ClientIp = "192.168.1.100"
};

// Multi-tier acquisition (Flink JobManager compatible - must pass ALL tiers)
if (multiTierLimiter.TryAcquire(context, permits: 100))
{
    // Allowed by all tiers - process messages
    try
    {
        await ProcessHighVolumeMessages(messages).ConfigureAwait(false);
    }
    catch (Exception ex)
    {
        _logger.LogError(ex, "High volume message processing failed");
    }
}

// Monitor utilization across tiers
var utilization = multiTierLimiter.GetUtilizationMetrics();
foreach (var tier in utilization)
{
    Console.WriteLine($"{tier.Key}: {tier.Value:P1} utilized");
}
```

### 4. Sliding Window Rate Limiter (Time-Based)

```csharp
// Precise time-based rate limiting: 5000 requests per 10-second window
var slidingLimiter = new SlidingWindowRateLimiter(
    maxRequestsPerSecond: 500.0,
    windowSizeSeconds: 10.0
);

// Process requests with time-based limiting (Flink JobManager compatible)
foreach (var request in apiRequests)
{
    if (slidingLimiter.TryAcquire())
    {
        try
        {
            await ProcessApiRequest(request).ConfigureAwait(false);
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "API request processing failed");
        }
    }
    else
    {
        await EnqueueForLater(request).ConfigureAwait(false);
    }
}

// Monitor real-time rate
Console.WriteLine($"Current rate: {slidingLimiter.ActualRate:F1} req/sec");
Console.WriteLine($"Window utilization: {slidingLimiter.CurrentUtilization:P1}");
```

### 5. Complete Integration Example

```csharp
public class FlinkAsyncSinkWithRateLimiting
{
    private readonly MultiTierRateLimiter _rateLimiter;
    private readonly BufferPool<SinkRecord> _bufferPool;
    
    public FlinkAsyncSinkWithRateLimiting(IConfiguration configuration)
    {
        // Configure connection to Aspire-managed containers
        var kafkaBootstrapServers = configuration["Kafka:BootstrapServers"] ?? "kafka-kraft-cluster:9092";
        var redisConnectionString = configuration["Redis:ConnectionString"] ?? "redis:6379";
        
        var kafkaConfig = new KafkaConfig
        {
            BootstrapServers = kafkaBootstrapServers
        };
        
        // Configure multi-tier rate limiting with Aspire backend
        _rateLimiter = RateLimiterFactory.CreateMultiTierWithKafkaStorage(kafkaConfig);
        _rateLimiter.ConfigureTiers(GetProductionTiers());
        
        // Configure buffer pool with thresholds
        _bufferPool = new BufferPool<SinkRecord>(
            maxSize: 10000,                    // Size threshold
            maxAge: TimeSpan.FromSeconds(5),   // Time threshold  
            rateLimiter: RateLimiterFactory.CreateWithKafkaStorage(1000, 2000, kafkaConfig)
        );
        
        // Handle flush events
        _bufferPool.OnFlush += FlushToExternalSystem;
        _bufferPool.OnBackpressure += HandleBackpressure;
    }
    
    public async Task WriteAsync(SinkRecord record)
    {
        var context = new RateLimitingContext
        {
            TopicName = record.Topic,
            ConsumerId = Environment.MachineName,
            ClientIp = GetLocalIpAddress()
        };
        
        // Multi-tier rate limiting check (Flink JobManager compatible)
        if (_rateLimiter.TryAcquire(context))
        {
            // Add to buffer (with size/time thresholds)
            await _bufferPool.AddAsync(record).ConfigureAwait(false);
        }
        else
        {
            // Apply backpressure to upstream
            throw new RateLimitExceededException("Multi-tier rate limit exceeded");
        }
    }
    
    private async Task FlushToExternalSystem(BufferedItem<SinkRecord>[] items)
    {
        try
        {
            // Batch write to external system (database, API, file, etc.)
            var records = items.Select(i => i.Item).ToList();
            await ExternalSystem.BatchWriteAsync(records).ConfigureAwait(false);
            
            Console.WriteLine($"Flushed {records.Count} records to external system");
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Batch write to external system failed");
            throw;
        }
    }
    
    private void HandleBackpressure(BackpressureEvent evt)
    {
        Console.WriteLine($"Backpressure triggered: {evt.Reason}");
        
        // Notify upstream components to slow down
        NotifyUpstreamComponents(evt);
        
        // Optionally adjust rate limits dynamically
        if (evt.Utilization > 0.9)
        {
            _rateLimiter.UpdateRateLimit("Consumer", 
                _rateLimiter.GetUtilizationMetrics()["Consumer"] * 0.8);
        }
    }
}

// Dependency Injection Setup (Program.cs or Startup.cs)
public class Program
{
    public static void Main(string[] args)
    {
        var builder = WebApplication.CreateBuilder(args);
        
        // Configure connection strings for Aspire containers
        builder.Configuration.AddJsonFile("appsettings.json");
        
        // Register rate limiting service
        builder.Services.AddSingleton<FlinkAsyncSinkWithRateLimiting>();
        
        var app = builder.Build();
        app.Run();
    }
}
```

### üîß Configuration Files for Aspire Integration

**appsettings.json:**
```json
{
  "Logging": {
    "LogLevel": {
      "Default": "Information"
    }
  },
  "Aspire": {
    "Kafka": {
      "BootstrapServers": "kafka-kraft-cluster:9092",
      "TopicName": "rate-limiter-state"
    },
    "Redis": {
      "ConnectionString": "redis:6379",
      "Database": 0
    },
    "Flink": {
      "JobManagerUrl": "http://flink-jobmanager:8081",
      "JobGatewayUrl": "http://flink-job-gateway:8080"
    }
  }
}
```

**Docker Compose Override (if needed):**
```yaml
version: '3.8'
services:
  your-app:
    image: your-app:latest
    environment:
      - Aspire__Kafka__BootstrapServers=kafka-kraft-cluster:9092
      - Aspire__Redis__ConnectionString=redis:6379
      - Aspire__Flink__JobManagerUrl=http://flink-jobmanager:8081
    depends_on:
      - kafka-kraft-cluster
      - redis
      - flink-jobmanager
```

### 6. Testing Your Implementation

```bash
# Run the comprehensive rate limiting demo
cd FlinkDotNet/Flink.JobBuilder
dotnet run --project Demo/RateLimitingDemo.cs

# Run integration tests to validate implementation
cd Sample/FlinkDotNet.Aspire.IntegrationTests
dotnet test --filter "Category=backpressure_test" --verbosity normal
```

### Key Benefits of This Implementation

‚úÖ **Follows Flink 2.0 Best Practices**: Based on official AsyncSink patterns  
‚úÖ **Multi-Tier Enforcement**: Global, Topic, Consumer, and Endpoint levels  
‚úÖ **Size AND Time Thresholds**: Buffer pool supports both triggers  
‚úÖ **Burst Accommodation**: Token bucket handles traffic spikes gracefully  
‚úÖ **Backpressure Integration**: Automatic upstream flow control  
‚úÖ **Production Ready**: Thread-safe, performance optimized, fully tested  

---

## Overview

Flink.NET uses .NET Aspire to orchestrate a complete distributed backpressure system across multiple containers. This architecture ensures high-throughput message processing (>900K msg/sec) while maintaining stability through sophisticated flow control mechanisms.

### System Architecture at a Glance

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                          FLINK.NET ASPIRE ARCHITECTURE                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ   KAFKA CLUSTER ‚îÇ    ‚îÇ FLINK CLUSTER   ‚îÇ    ‚îÇ     JOB GATEWAY             ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  (KRaft Mode)   ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ   (.NET Service)            ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                 ‚îÇ    ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ    ‚îÇ                             ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ 100 Partitions‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ ‚îÇ JobManager  ‚îÇ ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ ‚Ä¢ Backpressure Control     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ 8GB Memory    ‚îÇ    ‚îÇ ‚îÇ (Port 8081) ‚îÇ ‚îÇ    ‚îÇ ‚Ä¢ Rate Limiting             ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ KRaft Leader  ‚îÇ    ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ    ‚îÇ ‚Ä¢ Health Monitoring         ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Topic Auto-   ‚îÇ    ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ    ‚îÇ ‚Ä¢ API Gateway               ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   Creation      ‚îÇ    ‚îÇ ‚îÇTaskManager  ‚îÇ ‚îÇ    ‚îÇ ‚Ä¢ Swagger UI                ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Consumer      ‚îÇ    ‚îÇ ‚îÇ (Workers)   ‚îÇ ‚îÇ    ‚îÇ                             ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   Groups        ‚îÇ    ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ    ‚îÇ                             ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ           ‚îÇ                        ‚îÇ                           ‚îÇ                ‚îÇ
‚îÇ           ‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ                ‚îÇ
‚îÇ           ‚îÇ              ‚îÇ     REDIS       ‚îÇ                  ‚îÇ                ‚îÇ
‚îÇ           ‚îÇ              ‚îÇ   (State Store) ‚îÇ                  ‚îÇ                ‚îÇ
‚îÇ           ‚îÇ              ‚îÇ                 ‚îÇ                  ‚îÇ                ‚îÇ
‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ ‚Ä¢ Sequence Gen  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îÇ                          ‚îÇ ‚Ä¢ Counters      ‚îÇ                                   ‚îÇ
‚îÇ                          ‚îÇ ‚Ä¢ Rate Limits   ‚îÇ                                   ‚îÇ
‚îÇ                          ‚îÇ ‚Ä¢ Health State  ‚îÇ                                   ‚îÇ
‚îÇ                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                   ‚îÇ
‚îÇ                                    ‚îÇ                                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ                         ASPIRE ORCHESTRATION                           ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                                         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Container Lifecycle Management                                       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Service Discovery & Load Balancing                                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Health Check Coordination                                            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Network Topology & Port Management                                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Configuration Distribution                                           ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Monitoring & Observability Dashboard                                 ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## How Aspire Project Starts

### The Entry Point: Program.cs

The Aspire project starts through a single entry point in [`Sample/FlinkDotNet.Aspire.AppHost/Program.cs`](../../../Sample/FlinkDotNet.Aspire.AppHost/Program.cs):

```csharp
using Aspire.Hosting;

var builder = DistributedApplication.CreateBuilder(args);
// ... container configurations ...
builder.Build().Run();
```

### Starting the Complete Environment

**Command to start everything:**
```bash
cd Sample/FlinkDotNet.Aspire.AppHost
dotnet run
```

This single command orchestrates the entire distributed system with proper container startup sequencing and dependency management.

### Container Startup Sequence

Aspire manages the container startup in this order:

1. **Redis** - State store foundation
2. **Kafka Cluster** - Message backbone  
3. **Flink JobManager** - Cluster coordination
4. **Flink TaskManager** - Worker processes
5. **Flink Job Gateway** - .NET API service
6. **Kafka UI** - Monitoring interface
7. **Topic Initialization** - Automatic setup

## Container Architecture

### 1. Kafka Cluster (KRaft Mode)

**Configuration in Program.cs:**
```csharp
var kafka = builder.AddContainer("kafka-kraft-cluster", "bitnami/kafka:latest")
    .WithEndpoint(9092, 9092, "kafka")
    .WithEnvironment("KAFKA_ENABLE_KRAFT", "yes")  // No Zookeeper needed
    .WithEnvironment("KAFKA_CFG_NUM_PARTITIONS", "100")  // High partition count
    .WithEnvironment("KAFKA_HEAP_OPTS", "-Xmx8G -Xms8G")  // 8GB memory
    .WithEnvironment("KAFKA_CFG_NUM_IO_THREADS", "64")  // High concurrency
```

**Key Features:**
- **KRaft Mode**: No Zookeeper dependency, faster metadata operations
- **100 Default Partitions**: Supports massive parallelism  
- **8GB Memory**: Handles high-throughput processing
- **64 I/O Threads**: Optimized for concurrent message handling
- **Auto Topic Creation**: Dynamic topic provisioning

**Backpressure Role:**
- **Source Control**: Manages message ingestion rates
- **Partition Balancing**: Distributes load across consumers
- **Consumer Lag Monitoring**: Triggers backpressure signals
- **DLQ Management**: Handles failed message routing

### 2. Flink Cluster (JobManager + TaskManager)

**JobManager Configuration:**
```csharp
var flinkJobManager = builder.AddContainer("flink-jobmanager", "flink:2.0-SNAPSHOT")
    .WithHttpEndpoint(8081, 8081, "jobmanager-ui")
    .WithEnvironment("JOB_MANAGER_RPC_ADDRESS", "flink-jobmanager")
    .WithArgs("jobmanager");
```

**TaskManager Configuration:**
```csharp
var flinkTaskManager = builder.AddContainer("flink-taskmanager", "flink:2.0-SNAPSHOT")
    .WithEnvironment("JOB_MANAGER_RPC_ADDRESS", "flink-jobmanager")
    .WithArgs("taskmanager");
```

**Flink Cluster Role in Backpressure:**
- **JobManager**: Coordinates backpressure metrics across all TaskManagers
- **TaskManager**: Reports local buffer utilization and processing rates
- **Credit-Based Flow Control**: Manages downstream buffer allocation
- **Checkpoint Coordination**: Ensures consistent state during backpressure events

### 3. Flink Job Gateway (.NET Service)

**Configuration:**
```csharp
var flinkJobGateway = builder.AddContainer("flink-job-gateway", "flink-job-gateway")
    .WithHttpEndpoint(8080, 8080, "http")
    .WithEnvironment("FLINK_JOBMANAGER_URL", "http://flink-jobmanager:8081")
    .WithEnvironment("KAFKA_BOOTSTRAP_SERVERS", "kafka-kraft-cluster:9092")
    .WithEnvironment("REDIS_CONNECTION_STRING", $"{redis.GetEndpoint("tcp")}");
```

**Gateway Role in Backpressure:**
- **Rate Limiting**: API-level throttling and admission control
- **Health Monitoring**: Aggregates system health across all containers
- **Job Submission Control**: Prevents overloading during high backpressure
- **Metrics Collection**: Centralizes backpressure metrics from all sources

### 4. Redis (State Management)

**Configuration:**
```csharp
var redis = builder.AddRedis("redis");
```

**Redis Role in Backpressure:**
- **Sequence Generation**: Ensures message ordering during backpressure
- **Rate Limit Counters**: Tracks consumption rates across all consumers
- **Health State Storage**: Maintains cluster health information
- **Circuit Breaker State**: Coordinates failure detection and recovery

### 5. Kafka UI (Monitoring)

**Configuration:**
```csharp
var kafkaUI = builder.AddContainer("kafka-ui", "provectuslabs/kafka-ui:latest")
    .WithHttpEndpoint(8080, 8080, "kafka-ui")
    .WithEnvironment("KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS", "kafka-kraft-cluster:9092");
```

**Monitoring Role:**
- **Real-time Metrics**: Visualizes backpressure across topics and partitions
- **Consumer Lag Tracking**: Shows exactly where backpressure is occurring
- **Throughput Analysis**: Helps identify bottlenecks in the pipeline

## ReqNRoll BDD Testing with Aspire

### How ReqNRoll Uses Aspire for Container Orchestration

ReqNRoll (formerly SpecFlow) integrates with .NET Aspire through the `Aspire.Hosting.Testing` package to create isolated, containerized test environments for each BDD scenario. This ensures that backpressure tests run in production-like conditions with full container orchestration.

#### BDD Test Container Lifecycle

```csharp
// Each BDD scenario gets its own Aspire-managed container environment
[Binding]
public class BackpressureTestStepDefinitions
{
    private readonly AspireTesting.DistributedApplicationTestingBuilder _builder;
    private readonly AspireTesting.DistributedApplication _app;
    
    public BackpressureTestStepDefinitions()
    {
        // ReqNRoll creates fresh Aspire containers for each scenario
        _builder = AspireTesting.CreateDistributedApplicationBuilder();
        _app = ConfigureTestContainers(_builder);
    }
    
    private AspireTesting.DistributedApplication ConfigureTestContainers(
        AspireTesting.DistributedApplicationTestingBuilder builder)
    {
        // Same container setup as production, but isolated per test
        var kafka = builder.AddContainer("kafka-test", "bitnami/kafka:latest")
            .WithEnvironment("KAFKA_ENABLE_KRAFT", "yes")
            .WithEnvironment("KAFKA_CFG_NUM_PARTITIONS", "100");
            
        var flink = builder.AddContainer("flink-jobmanager-test", "flink:2.0-SNAPSHOT")
            .WithArgs("jobmanager");
            
        var redis = builder.AddRedis("redis-test");
        
        return builder.Build();
    }
}
```

#### Test-Specific Container Isolation

Each BDD scenario runs in completely isolated containers:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    REQNROLL BDD TEST ISOLATION                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                         ‚îÇ
‚îÇ  Scenario 1: Consumer Lag Test        Scenario 2: Network Test          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ  ‚îÇ   Isolated Containers   ‚îÇ          ‚îÇ   Isolated Containers   ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ          ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚îÇKafka‚îÇ ‚îÇFlink‚îÇ ‚îÇJob ‚îÇ ‚îÇ          ‚îÇ  ‚îÇKafka‚îÇ ‚îÇFlink‚îÇ ‚îÇJob ‚îÇ ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚îÇTest ‚îÇ ‚îÇTest ‚îÇ ‚îÇGW  ‚îÇ ‚îÇ          ‚îÇ  ‚îÇTest ‚îÇ ‚îÇTest ‚îÇ ‚îÇGW  ‚îÇ ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ:9092‚îÇ ‚îÇ:8081‚îÇ ‚îÇTest‚îÇ ‚îÇ          ‚îÇ  ‚îÇ:9093‚îÇ ‚îÇ:8082‚îÇ ‚îÇTest‚îÇ ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ          ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ  Each test scenario gets fresh containers with unique ports             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### ReqNRoll Feature Files with Aspire Integration

The BDD scenarios directly interact with Aspire-managed containers:

```gherkin
@backpressure_test @aspire_containers
Feature: Backpressure Test - Consumer Lag-Based Flow Control

  Background:
    Given the Aspire test environment is initialized with fresh containers
    And Kafka test cluster is started with 100 partitions  
    And Flink test cluster is connected to Kafka test cluster
    And Redis test instance is available for state management
    And all test containers are healthy and communicating

  @consumer_lag @aspire_isolation
  Scenario: Consumer Lag-Based Backpressure with Aspire Containers
    Given I have an Aspire-managed Kafka setup with test-specific topics:
      | Topic Name | Partitions | Replication | Purpose |
      | backpressure-input-test | 16 | 1 | Test input stream |
      | backpressure-output-test | 8 | 1 | Test output stream |
      | backpressure-dlq-test | 4 | 1 | Test dead letter queue |
    When I produce 100,000 test messages through Aspire container network
    And I configure consumer lag monitoring with 2-second test intervals
    Then the Aspire-managed containers should demonstrate backpressure:
      | Container | Backpressure Behavior | Validation Method |
      | Kafka Test | Topic lag increases, partitions throttle | Container log analysis |
      | Flink Test | Consumer group rebalancing triggered | JobManager metrics |
      | Job Gateway Test | Rate limiting activated | Health endpoint status |
      | Redis Test | Backpressure state persisted | Cache key validation |
```

#### Step Definitions with Aspire Container Management

```csharp
[Given(@"the Aspire test environment is initialized with fresh containers")]
public async Task GivenAspireTestEnvironmentInitialized()
{
    _output.WriteLine("üèóÔ∏è Initializing Aspire test containers...");
    
    // Start all containers in parallel using Aspire orchestration
    await _app.StartAsync().ConfigureAwait(false);
    
    // Wait for all containers to be healthy
    var healthChecks = new[]
    {
        WaitForContainerHealthy("kafka-test", TimeSpan.FromSeconds(30)),
        WaitForContainerHealthy("flink-jobmanager-test", TimeSpan.FromSeconds(20)),
        WaitForContainerHealthy("redis-test", TimeSpan.FromSeconds(10))
    };
    
    await Task.WhenAll(healthChecks);
    
    _output.WriteLine("‚úÖ All Aspire test containers are healthy and ready");
}

[When(@"I produce ([\d,]+) test messages through Aspire container network")]
public async Task WhenIProduceTestMessagesThroughAspireNetwork(int messageCount)
{
    _output.WriteLine($"üì§ Producing {messageCount:N0} messages through Aspire container network");
    
    // Get Kafka endpoint from Aspire container
    var kafkaEndpoint = _app.GetResource("kafka-test").GetEndpoint("kafka");
    var kafkaBootstrapServers = $"{kafkaEndpoint.Host}:{kafkaEndpoint.Port}";
    
    // Create producer using Aspire-managed Kafka
    var producer = new KafkaProducer(kafkaBootstrapServers);
    
    // Send messages and validate through container network
    var sent = await producer.SendMessages("backpressure-input-test", messageCount);
    Assert.True(sent, "Messages should be sent through Aspire container network");
    
    _output.WriteLine($"‚úÖ {messageCount:N0} messages sent through Aspire containers");
}

[Then(@"the Aspire-managed containers should demonstrate backpressure:")]
public async Task ThenAspireManagedContainersShouldDemonstrateBackpressure(Table table)
{
    _output.WriteLine("üîç Validating backpressure in Aspire-managed containers...");
    
    foreach (var row in table.Rows)
    {
        var container = row["Container"];
        var behavior = row["Backpressure Behavior"];
        var validation = row["Validation Method"];
        
        switch (container)
        {
            case "Kafka Test":
                await ValidateKafkaBackpressureInContainer(behavior, validation);
                break;
            case "Flink Test":
                await ValidateFlinkBackpressureInContainer(behavior, validation);
                break;
            case "Job Gateway Test":
                await ValidateJobGatewayBackpressureInContainer(behavior, validation);
                break;
            case "Redis Test":
                await ValidateRedisBackpressureInContainer(behavior, validation);
                break;
        }
        
        _output.WriteLine($"  ‚úÖ {container}: {behavior} validated via {validation}");
    }
    
    _output.WriteLine("‚úÖ Backpressure validated across all Aspire containers");
}
```

#### Container Health Monitoring During Tests

```csharp
private async Task<bool> WaitForContainerHealthy(string containerName, TimeSpan timeout)
{
    var container = _app.GetResource(containerName);
    var startTime = DateTime.UtcNow;
    
    while (DateTime.UtcNow - startTime < timeout)
    {
        try
        {
            // Check container health through Aspire resource management
            var health = await container.GetHealthAsync().ConfigureAwait(false);
            if (health.Status == AspireTesting.HealthStatus.Healthy)
            {
                _output.WriteLine($"‚úÖ Container {containerName} is healthy");
                return true;
            }
            
            _output.WriteLine($"‚è≥ Waiting for {containerName} to be healthy...");
            await Task.Delay(TimeSpan.FromSeconds(2));
        }
        catch (Exception ex)
        {
            _output.WriteLine($"‚ö†Ô∏è Health check failed for {containerName}: {ex.Message}");
            await Task.Delay(TimeSpan.FromSeconds(1));
        }
    }
    
    throw new TimeoutException($"Container {containerName} did not become healthy within {timeout}");
}
```

#### Backpressure Validation in Aspire Containers

```csharp
private async Task ValidateKafkaBackpressureInContainer(string behavior, string validation)
{
    var kafkaContainer = _app.GetResource("kafka-test");
    
    switch (validation)
    {
        case "Container log analysis":
            // Read container logs through Aspire
            var logs = await kafkaContainer.GetLogsAsync().ConfigureAwait(false);
            var hasThrottling = logs.Contains("throttling") || logs.Contains("lag");
            Assert.True(hasThrottling, "Kafka container should show throttling in logs");
            break;
            
        case "Topic metrics validation":
            // Connect to Kafka through Aspire container network
            var kafkaEndpoint = kafkaContainer.GetEndpoint("kafka");
            var adminClient = new KafkaAdminClient($"{kafkaEndpoint.Host}:{kafkaEndpoint.Port}");
            
            var topicMetrics = await adminClient.GetTopicMetrics("backpressure-input-test");
            Assert.True(topicMetrics.ConsumerLag > 0, "Consumer lag should be present");
            break;
    }
}

private async Task ValidateFlinkBackpressureInContainer(string behavior, string validation)
{
    var flinkContainer = _app.GetResource("flink-jobmanager-test");
    
    switch (validation)
    {
        case "JobManager metrics":
            // Access Flink JobManager through Aspire container
            var flinkEndpoint = flinkContainer.GetEndpoint("jobmanager-ui");
            var jobManagerClient = new FlinkJobManagerClient($"http://{flinkEndpoint.Host}:{flinkEndpoint.Port}");
            
            var metrics = await jobManagerClient.GetMetrics();
            Assert.True(metrics.BackpressureRatio > 0, "Flink should show backpressure ratio > 0");
            break;
            
        case "Consumer group rebalancing":
            // Validate rebalancing through container network
            var rebalanceEvents = await jobManagerClient.GetRebalanceEvents();
            Assert.True(rebalanceEvents.Any(), "Consumer group rebalancing should occur");
            break;
    }
}
```

#### Test Cleanup and Container Disposal

```csharp
[AfterScenario]
public async Task CleanupAspireContainers()
{
    _output.WriteLine("üßπ Cleaning up Aspire test containers...");
    
    try
    {
        // Aspire automatically handles container cleanup
        await _app.DisposeAsync().ConfigureAwait(false);
        _output.WriteLine("‚úÖ All test containers cleaned up successfully");
    }
    catch (Exception ex)
    {
        _output.WriteLine($"‚ö†Ô∏è Container cleanup warning: {ex.Message}");
    }
}
```

#### Key Benefits of ReqNRoll + Aspire Integration

**üîÑ Complete Test Isolation**
- Each BDD scenario runs in fresh containers
- No test contamination or shared state issues
- Production-like container environment for every test

**üìä Real Container Validation** 
- Tests actual container-to-container communication
- Validates real network latency and container startup times
- Tests true service discovery and container health checks

**üéØ Backpressure Testing Accuracy**
- Tests backpressure in actual container environment
- Validates container resource limits and scaling
- Tests real network partitions and container failures

**‚ö° Parallel Test Execution**
- Multiple BDD scenarios run simultaneously with isolated containers
- Aspire manages port allocation and container networking automatically
- Faster test feedback with true parallelization

**üõ†Ô∏è Production Parity**
- Test containers use same configuration as production
- Same Aspire orchestration patterns
- Identical container network topology

This integration ensures that backpressure behavior tested in BDD scenarios exactly matches production behavior, as both use identical Aspire container orchestration patterns.

## Backpressure Flow Between Containers

### Multi-Container Backpressure Coordination

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      CONTAINER-TO-CONTAINER BACKPRESSURE FLOW                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                                 ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚ù∂  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚ù∑  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ ‚îÇ KAFKA CLUSTER   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ FLINK CLUSTER   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   JOB GATEWAY               ‚îÇ ‚îÇ
‚îÇ ‚îÇ                 ‚îÇ     ‚îÇ                 ‚îÇ     ‚îÇ                             ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚Ä¢ Lag Detection ‚îÇ     ‚îÇ ‚Ä¢ Buffer Monitor‚îÇ     ‚îÇ ‚Ä¢ Rate Limit Decision       ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚Ä¢ Partition     ‚îÇ     ‚îÇ ‚Ä¢ Credit System ‚îÇ     ‚îÇ ‚Ä¢ Health Aggregation        ‚îÇ ‚îÇ
‚îÇ ‚îÇ   Pressure      ‚îÇ     ‚îÇ ‚Ä¢ Checkpoint    ‚îÇ     ‚îÇ ‚Ä¢ API Throttling            ‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ   Coordination  ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ          ‚ñ≤              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îÇ              ‚îÇ
‚îÇ          ‚îÇ                        ‚îÇ                             ‚îÇ              ‚îÇ
‚îÇ          ‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                     ‚îÇ              ‚îÇ
‚îÇ          ‚îÇ              ‚îÇ     REDIS       ‚îÇ                     ‚îÇ              ‚îÇ
‚îÇ          ‚îÇ              ‚îÇ                 ‚îÇ                     ‚îÇ              ‚îÇ
‚îÇ          ‚îÇ              ‚îÇ ‚Ä¢ Rate Counters ‚îÇ                     ‚îÇ              ‚îÇ
‚îÇ          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ ‚Ä¢ Health State  ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
‚îÇ                         ‚îÇ ‚Ä¢ Circuit       ‚îÇ                                    ‚îÇ
‚îÇ                         ‚îÇ   Breaker       ‚îÇ                                    ‚îÇ
‚îÇ                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                    ‚îÇ
‚îÇ                                                                                 ‚îÇ
‚îÇ ‚ù∂ Kafka ‚Üí Flink: Consumer lag triggers Flink buffer management                 ‚îÇ
‚îÇ ‚ùÇ Flink ‚Üí Gateway: Buffer utilization affects job submission rates             ‚îÇ
‚îÇ ‚ù∏ Gateway ‚Üí Redis: Stores rate decisions and health state                      ‚îÇ
‚îÇ ‚ùπ Redis ‚Üí Kafka: Influences partition assignment and consumer rates            ‚îÇ
‚îÇ                                                                                 ‚îÇShot
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Step-by-Step Backpressure Flow

**Phase 1: Detection (Kafka ‚Üí Flink)**
1. **Kafka** detects consumer lag on specific partitions
2. **Consumer Group Coordinator** signals partition pressure
3. **Flink TaskManager** receives partition assignment with lag info
4. **Credit-based flow control** reduces downstream message requests

**Phase 2: Propagation (Flink ‚Üí Gateway)**  
1. **Flink JobManager** aggregates buffer utilization from all TaskManagers
2. **Checkpoint barriers** coordinate backpressure across parallel operations
3. **Job Gateway** receives cluster health metrics via Flink REST API
4. **Rate limiting decisions** made based on cluster capacity

**Phase 3: Coordination (Gateway ‚Üí Redis)**
1. **Rate limit decisions** stored in Redis with TTL
2. **Health state updates** propagated to all connected services  
3. **Circuit breaker states** coordinated across multiple instances
4. **Sequence counters** adjusted for reduced throughput

**Phase 4: Feedback (Redis ‚Üí Kafka)**
1. **Consumer rate limits** retrieved from Redis during polling
2. **Partition rebalancing** triggered based on stored health state
3. **Auto-scaling decisions** influence topic partition allocation
4. **Dead letter queue routing** activated for failed messages

## Container Interaction Patterns

### 1. Service Discovery and Communication

**Inter-Container Communication:**
```csharp
// Gateway finds Flink JobManager
.WithEnvironment("FLINK_JOBMANAGER_URL", "http://flink-jobmanager:8081")

// Gateway connects to Kafka
.WithEnvironment("KAFKA_BOOTSTRAP_SERVERS", "kafka-kraft-cluster:9092")

// All services use Redis
.WithEnvironment("REDIS_CONNECTION_STRING", $"{redis.GetEndpoint("tcp")}")
```

**Network Topology:**
- **Service Mesh**: Aspire creates internal Docker network for all containers
- **DNS Resolution**: Container names resolve to IP addresses automatically
- **Port Management**: Dynamic port allocation with health checks
- **Load Balancing**: Built-in load balancing for multiple TaskManager instances

### 2. Health Check Coordination

**Multi-Service Health Monitoring:**
```csharp
// Each container reports health to Aspire
// Aspire dashboard aggregates all health states
// Failed containers trigger automatic restart
// Backpressure system adapts to container failures
```

**Health Check Flow:**
1. **Individual Health**: Each container reports its own health status
2. **Service Health**: Aspire aggregates container health per service
3. **System Health**: Overall system health influences backpressure decisions
4. **Auto-Recovery**: Failed containers automatically restart with preserved state

### 3. Configuration Distribution

**Environment-Based Configuration:**
```csharp
// Shared configuration across all containers
var kafkaEndpoint = "kafka-kraft-cluster:9092";
var redisEndpoint = redis.GetEndpoint("tcp");

// Each container receives relevant configuration
.WithEnvironment("KAFKA_BOOTSTRAP_SERVERS", kafkaEndpoint)
.WithEnvironment("REDIS_CONNECTION_STRING", redisEndpoint)
```

## Monitoring and Observability

### Aspire Dashboard Integration

**Access the Dashboard:**
```bash
# Start Aspire (shows dashboard URL in output)
cd Sample/FlinkDotNet.Aspire.AppHost
dotnet run

# Typically available at: http://localhost:15000
```

**Dashboard Features:**
- **Service Overview**: All containers with health status
- **Resource Utilization**: CPU, memory, network per container
- **Log Aggregation**: Centralized logs from all services
- **Metrics Collection**: Real-time performance metrics
- **Endpoint Management**: All service endpoints in one place

### Container-Specific Monitoring

**Kafka UI** (Access via Aspire Dashboard):
- Topic partition details and consumer lag
- Producer/consumer throughput metrics  
- Message routing and DLQ analysis

**Flink Web UI** (Port 8081):
- JobManager cluster overview
- TaskManager resource utilization
- Checkpoint and savepoint management
- Backpressure visualization

**Job Gateway Swagger** (Port 8080):
- API endpoint testing
- Health check status
- Rate limiting configuration

## Production Deployment

### Container Resource Requirements

**Minimum Production Setup:**
```yaml
# Resource allocation per container
Kafka Cluster:     8GB RAM, 4 CPU cores, 100GB SSD
Flink JobManager:  4GB RAM, 2 CPU cores, 20GB SSD  
Flink TaskManager: 8GB RAM, 4 CPU cores, 50GB SSD (per instance)
Job Gateway:       2GB RAM, 2 CPU cores, 10GB SSD
Redis:             4GB RAM, 2 CPU cores, 20GB SSD
Kafka UI:          1GB RAM, 1 CPU core, 5GB SSD
```

### Scaling Considerations

**Horizontal Scaling:**
- **TaskManager**: Multiple instances for increased parallelism
- **Job Gateway**: Multiple instances behind load balancer
- **Kafka Partitions**: Scale partitions based on expected throughput

**Vertical Scaling:**
- **Kafka Memory**: Increase for higher message throughput
- **Flink Memory**: Increase for larger state and more complex processing
- **Redis Memory**: Scale based on state storage requirements

### Production Configuration

**High-Availability Setup:**
```csharp
// Production-ready container configuration
.WithEnvironment("KAFKA_CFG_DEFAULT_REPLICATION_FACTOR", "3")  // Data durability
.WithEnvironment("KAFKA_CFG_MIN_INSYNC_REPLICAS", "2")        // Write consistency
.WithEnvironment("FLINK_JOBMANAGER_HEAP_SIZE", "4096m")       // Stable memory
.WithEnvironment("FLINK_TASKMANAGER_HEAP_SIZE", "8192m")      // Processing capacity
```

## Troubleshooting

### Common Container Issues

**Container Startup Problems:**
```bash
# Check Aspire dashboard for container status
# View individual container logs through dashboard
# Verify Docker resource allocation (CPU/Memory limits)
```

**Network Communication Issues:**
```bash
# Verify service discovery through Aspire dashboard
# Check endpoint accessibility for each container
# Validate environment variable configuration
```

**Performance Bottlenecks:**
```bash
# Monitor resource utilization in Aspire dashboard
# Check Kafka UI for partition lag and throughput
# Analyze Flink Web UI for backpressure indicators
```

### Backpressure-Specific Troubleshooting

**High Consumer Lag:**
1. Check TaskManager resource utilization in Flink UI
2. Verify partition distribution across consumers
3. Analyze message processing rates per topic
4. Consider scaling TaskManager instances

**Rate Limiting Issues:**
1. Monitor Job Gateway API metrics
2. Check Redis for rate limit counter states
3. Verify health check responses from all containers
4. Analyze circuit breaker status

**System Recovery:**
1. Use Aspire dashboard to restart individual containers
2. Monitor automatic health check recovery
3. Verify state consistency after container restart
4. Check message processing resume after recovery

## Integration with Existing Documentation

This wiki complements other Flink.NET documentation:

- **[Aspire Local Development Setup](./Aspire-Local-Development-Setup.md)**: Detailed setup instructions and testing procedures
- **[Backpressure Implementation Guide](./Backpressure-Implementation-Guide.md)**: Algorithmic details and implementation patterns  
- **[FLINK_NET_BACK_PRESSURE.md](./FLINK_NET_BACK_PRESSURE.md)**: Technical implementation specifications

## Next Steps

1. **Set up the environment**: Follow the [Aspire Local Development Setup](./Aspire-Local-Development-Setup.md) guide
2. **Run backpressure tests**: Execute the integration tests to see the system in action
3. **Explore the code**: Review the Program.cs file and container configurations
4. **Monitor in real-time**: Use the Aspire dashboard and Kafka UI to observe backpressure behavior
5. **Scale for production**: Apply the production deployment guidelines for your environment

## Kafka Partitioning Strategy Analysis

### Overview: Space vs Time Trade-offs in Partition Architecture

This section provides a comprehensive analysis of Kafka partitioning strategies as they relate to backpressure implementation, based on production experience from LinkedIn, Confluent, and other industry deployments.

**Core Trade-off:** Every Kafka partitioning strategy involves balancing **space** (resource utilization per partition) vs **time** (processing latency and throughput).

### Standard Partitioning Approach (Recommended Production Pattern)

#### ‚úÖ **Advantages (Pros)**

**üè¢ Space: Per Partition, Not Per Logical Queue**
- **Implementation**: [`MultiTierRateLimiter.cs:61-102`](../../FlinkDotNet/Flink.JobBuilder/Backpressure/MultiTierRateLimiter.cs)
- **Behavior**: Rate limits are enforced at partition level, allowing multiple logical message queues to share partition resources
- **Code Example**:
```csharp
// Multiple logical queues can exist within a single partition
// Rate limiting is applied per partition, not per logical message type
var context = new RateLimitingContext 
{
    TopicName = "high-volume-topic",    // Physical partition assignment
    LogicalQueue = "order-processing",   // Logical separation within partition
    ConsumerId = "consumer-group-1"
};
```
- **Test Coverage**: [`BackpressureTest.feature:144-186`](../../Sample/FlinkDotNet.Aspire.IntegrationTests/Features/BackpressureTest.feature) - Rate limiting with finite resources scenario

**‚ö° Scalability: Millions of Logical Queues Over Finite Partitions**
- **Implementation**: [`KafkaRateLimiterStateStorage.cs:85-120`](../../FlinkDotNet/Flink.JobBuilder/Backpressure/KafkaRateLimiterStateStorage.cs)
- **Behavior**: Kafka's consistent hashing allows millions of logical message types to be distributed across a finite number of partitions (typically 16-128 per topic)
- **Code Example**:
```csharp
// Millions of logical queues distributed across finite partitions
var partitionKey = CalculatePartition(logicalQueueId, totalPartitions);
await _kafkaStorage.SetRateLimitStateAsync($"logical-queue-{logicalQueueId}", 
    partitionKey, rateLimitState).ConfigureAwait(false);
```
- **Test Coverage**: [`BackpressureTestStepDefinitions.cs:892-935`](../../Sample/FlinkDotNet.Aspire.IntegrationTests/StepDefinitions/BackpressureTestStepDefinitions.cs) - ValidateScalabilityWithMillionsOfLogicalQueues

**üîí Operational Isolation: Separate Clusters for Business Domains**
- **Implementation**: Multi-cluster configuration in [`Program.cs:25-45`](../../Sample/FlinkDotNet.Aspire.AppHost/Program.cs)
- **Behavior**: Different business domains (orders, payments, analytics) use separate Kafka clusters to prevent cross-contamination
- **Code Example**:
```csharp
// Separate clusters for different business domains
var orderCluster = builder.AddContainer("kafka-orders", "bitnami/kafka:latest")
    .WithEnvironment("KAFKA_CFG_NUM_PARTITIONS", "32");
var paymentCluster = builder.AddContainer("kafka-payments", "bitnami/kafka:latest") 
    .WithEnvironment("KAFKA_CFG_NUM_PARTITIONS", "16");
```
- **Test Coverage**: [`BackpressureTest.feature:15-47`](../../Sample/FlinkDotNet.Aspire.IntegrationTests/Features/BackpressureTest.feature) - Multi-cluster setup with business domain isolation

**üìä Quota Enforcement: Producer/Consumer Quotas at Client, User, IP Level**
- **Implementation**: [`MultiTierRateLimiter.cs:245-285`](../../FlinkDotNet/Flink.JobBuilder/Backpressure/MultiTierRateLimiter.cs)
- **Behavior**: Hierarchical rate limiting enforces quotas at multiple levels: client ‚Üí user ‚Üí IP ‚Üí topic ‚Üí partition
- **Code Example**:
```csharp
// Multi-level quota enforcement
var tiers = new[] {
    new RateLimitingTier { Name = "Global", RateLimit = 10_000_000 },
    new RateLimitingTier { Name = "Client", RateLimit = 1_000_000 },
    new RateLimitingTier { Name = "User", RateLimit = 100_000 },
    new RateLimitingTier { Name = "IP", RateLimit = 50_000 }
};
```
- **Test Coverage**: [`BackpressureTestStepDefinitions.cs:1245-1285`](../../Sample/FlinkDotNet.Aspire.IntegrationTests/StepDefinitions/BackpressureTestStepDefinitions.cs) - ValidateQuotaEnforcementAtAllLevels

**üîÑ Dynamic Scaling: Consumer Lag Monitoring and Auto-scaling**
- **Implementation**: [`BufferPool.cs:125-165`](../../FlinkDotNet/Flink.JobBuilder/Backpressure/BufferPool.cs)
- **Behavior**: Continuous consumer lag monitoring triggers automatic rebalancing and consumer scaling
- **Code Example**:
```csharp
// Dynamic scaling based on consumer lag
bufferPool.OnBackpressure += async evt => {
    if (evt.ConsumerLag > scalingThreshold) {
        await autoScaler.AddConsumerInstance();
    }
};
```
- **Test Coverage**: [`BackpressureTest.feature:14-47`](../../Sample/FlinkDotNet.Aspire.IntegrationTests/Features/BackpressureTest.feature) - Consumer lag-based dynamic scaling scenario

**üìã Operational Playbooks: Mature Procedures for Issue Resolution**
- **Implementation**: Documented in [`Troubleshooting Section:1140-1160`](#troubleshooting)
- **Behavior**: Established procedures for lag resolution, scaling decisions, and isolation problem-solving
- **Test Coverage**: [`BackpressureTest.feature:187-224`](../../Sample/FlinkDotNet.Aspire.IntegrationTests/Features/BackpressureTest.feature) - Complete integration test with comprehensive practices

#### ‚ùå **Disadvantages (Cons)**

**üîß Operational Complexity: Significant SRE Investment Required**
- **Challenge**: Requires substantial investment in automation, monitoring, quota management, dynamic scaling, and SRE practices
- **Implementation Impact**: Visible in complex configuration across multiple files: [`Program.cs`](../../Sample/FlinkDotNet.Aspire.AppHost/Program.cs), [`MultiTierRateLimiter.cs`](../../FlinkDotNet/Flink.JobBuilder/Backpressure/MultiTierRateLimiter.cs), [`KafkaRateLimiterStateStorage.cs`](../../FlinkDotNet/Flink.JobBuilder/Backpressure/KafkaRateLimiterStateStorage.cs)
- **Code Evidence**:
```csharp
// Complex multi-tier configuration required
var rateLimiter = new MultiTierRateLimiter(kafkaStorage);
rateLimiter.ConfigureTiers(multipleComplexTiers);
rateLimiter.EnableDynamicRebalancing(autoScalingConfig);
rateLimiter.SetupQuotaManagement(quotaConfig);
// + Monitoring, alerting, automation layers...
```
- **Operational Cost**: Higher hardware, networking, and personnel costs (SRE, tooling, monitoring, multi-cluster management)

**‚öñÔ∏è No True Per-Logical-Queue Rate Limiting**
- **Limitation**: Quotas are enforced per client/topic/cluster, not per logical queue within a partition
- **Implementation**: [`MultiTierRateLimiter.cs:61-85`](../../FlinkDotNet/Flink.JobBuilder/Backpressure/MultiTierRateLimiter.cs)
- **Impact**: Fine-grained fairness inside a partition requires custom logic or additional layers
- **Code Example**:
```csharp
// Rate limiting happens at partition level, not logical queue level
// Multiple logical queues in same partition share the rate limit
_rateLimiter.TryAcquire(partitionContext); // Per partition, not per logical queue (Flink JobManager compatible)
```
- **Test Coverage**: [`BackpressureTestStepDefinitions.cs:1385-1425`](../../Sample/FlinkDotNet.Aspire.IntegrationTests/StepDefinitions/BackpressureTestStepDefinitions.cs) - ValidateLogicalQueueFairnessLimitations

**‚ö° Short-term Noisy Neighbor Impact**
- **Challenge**: Rapid changes or spikes can cause temporary imbalances before automation reacts
- **Mitigation**: Strongly mitigated by quotas/isolation, but can't be completely eliminated
- **Implementation**: [`TokenBucketRateLimiter.cs:47-85`](../../FlinkDotNet/Flink.JobBuilder/Backpressure/TokenBucketRateLimiter.cs) provides burst accommodation to reduce impact
- **Code Example**:
```csharp
// Burst capacity helps but doesn't eliminate noisy neighbor impact
var rateLimiter = new TokenBucketRateLimiter(
    rateLimit: 1000.0,      // Sustained rate
    burstCapacity: 2000.0   // Temporary burst allowance
);
```
- **Test Coverage**: [`BackpressureTest.feature:110-143`](../../Sample/FlinkDotNet.Aspire.IntegrationTests/Features/BackpressureTest.feature) - Network-bound bottleneck handling with noisy neighbor simulation

**üîÑ Upgrade and Maintenance Overhead**
- **Challenge**: Multi-layered, multi-cluster setups require coordinated upgrades and maintenance
- **Impact**: More "moving parts" increase operational complexity
- **Evidence**: Aspire orchestration manages this complexity but doesn't eliminate it - see [`Program.cs:495-585`](../../Sample/FlinkDotNet.Aspire.AppHost/Program.cs)

**üìà Lag and Throughput Limits**
- **Limitation**: Per-partition speed limits (~100k msgs/sec) still apply
- **Solution**: Aggregate throughput requires scaling partitions and brokers
- **Implementation**: [`BackpressureTestStepDefinitions.cs:892-920`](../../Sample/FlinkDotNet.Aspire.IntegrationTests/StepDefinitions/BackpressureTestStepDefinitions.cs)
- **Code Example**:
```csharp
var baseLoad = partitions.Count * 50000; // 50K messages/sec per partition limit
// To achieve higher throughput, must increase partition count
```

### 1 Million+ Partition Single-Layer Approach

#### ‚úÖ **Advantages (Pros)**

**üõ°Ô∏è Best for Handling Noisy Neighbor**
- **Implementation**: Would require massive partition scaling in [`Program.cs`](../../Sample/FlinkDotNet.Aspire.AppHost/Program.cs)
- **Behavior**: Each logical queue gets its own partition, eliminating cross-contamination
- **Code Example**:
```csharp
// Theoretical implementation (not recommended)
var kafka = builder.AddContainer("kafka-massive", "bitnami/kafka:latest")
    .WithEnvironment("KAFKA_CFG_NUM_PARTITIONS", "1000000")  // 1M partitions
    .WithEnvironment("KAFKA_HEAP_OPTS", "-Xmx1000G");       // Massive memory required
```
- **Test Coverage**: No current test coverage - would require significant infrastructure

#### ‚ùå **Disadvantages (Cons)**

**üíæ 100% Space Problem with Finite Partitions**
- **Core Issue**: The fundamental problem becomes entirely about space allocation, while time performance is optimal
- **Implementation Impact**: [`KafkaRateLimiterStateStorage.cs`](../../FlinkDotNet/Flink.JobBuilder/Backpressure/KafkaRateLimiterStateStorage.cs) would need massive scaling
- **Resource Requirements**: 
```csharp
// Massive resource requirements for 1M+ partitions
// Memory: ~10GB per 100K partitions = 100GB+ for 1M partitions
// CPU: Linear scaling with partition count
// Network: Exponential growth in coordinator overhead
```

**üè¢ Cluster Limits: World-Class Deployment Recommendations**
- **LinkedIn Scale**: Recommends keeping partition counts in low hundreds of thousands per cluster
- **Confluent Cloud**: Similar limits for reliability and manageability
- **AWS MSK**: Performance degrades significantly beyond 300K partitions per cluster
- **Evidence**: Current implementation limits reflect these constraints in [`Program.cs:535-545`](../../Sample/FlinkDotNet.Aspire.AppHost/Program.cs)

**‚ö° Resource Inefficiency**
- **Challenge**: Many partitions remain underutilized, wasting resources
- **Impact**: Poor resource utilization across the cluster
- **Cost**: Dramatically higher infrastructure costs for marginal benefit
- **Implementation**: Would require custom partition management beyond current [`MultiTierRateLimiter.cs`](../../FlinkDotNet/Flink.JobBuilder/Backpressure/MultiTierRateLimiter.cs) capabilities

### Production Recommendations and Current Implementation

**‚úÖ Recommended: Standard Multi-Tier Approach**
- **Current Implementation**: Flink.NET uses standard partitioning with multi-tier rate limiting
- **Partition Count**: 100 partitions per topic (configurable in [`Program.cs:535`](../../Sample/FlinkDotNet.Aspire.AppHost/Program.cs))
- **Rate Limiting**: Multi-tier enforcement at Global ‚Üí Topic ‚Üí Consumer ‚Üí Endpoint levels
- **Storage**: Kafka-based distributed state storage for enterprise scale

**‚ùå Not Recommended: Million+ Partition Approach**
- **Reason**: Operational complexity and resource inefficiency outweigh noisy neighbor benefits
- **Alternative**: Use standard partitioning with robust quota enforcement and operational isolation

### Test Coverage for Partition Strategies

All partition strategy concepts are validated through comprehensive test scenarios:

1. **Standard Partitioning Tests**: [`BackpressureTest.feature:14-266`](../../Sample/FlinkDotNet.Aspire.IntegrationTests/Features/BackpressureTest.feature)
2. **Quota Enforcement Validation**: [`BackpressureTestStepDefinitions.cs:1200-1300`](../../Sample/FlinkDotNet.Aspire.IntegrationTests/StepDefinitions/BackpressureTestStepDefinitions.cs)
3. **Noisy Neighbor Handling**: [`BackpressureTest.feature:110-143`](../../Sample/FlinkDotNet.Aspire.IntegrationTests/Features/BackpressureTest.feature)
4. **Dynamic Scaling Integration**: [`BackpressureTestStepDefinitions.cs:1400-1500`](../../Sample/FlinkDotNet.Aspire.IntegrationTests/StepDefinitions/BackpressureTestStepDefinitions.cs)
5. **Operational Complexity Measurement**: [`BackpressureTest.feature:187-224`](../../Sample/FlinkDotNet.Aspire.IntegrationTests/Features/BackpressureTest.feature)

### Key Takeaways

**For Production Systems:**
- ‚úÖ Use standard partitioning (16-128 partitions per topic)
- ‚úÖ Implement multi-tier rate limiting for quota enforcement  
- ‚úÖ Use separate clusters for business domain isolation
- ‚úÖ Invest in operational tooling and SRE practices
- ‚ùå Avoid million+ partition approaches unless specific use case demands it

**Space vs Time Decision Matrix:**
- **Choose Standard Partitioning When**: Operational simplicity, cost efficiency, and established scalability are priorities
- **Consider Million+ Partitions When**: Perfect noisy neighbor isolation is critical AND you have unlimited operational resources
- **Reality**: Standard partitioning with proper quota enforcement achieves 99% of noisy neighbor benefits at 10% of the operational cost

---

*This wiki provides the complete picture of how Flink.NET Aspire orchestrates distributed backpressure. The combination of proper container architecture, service coordination, and monitoring creates a robust, production-ready streaming platform.*